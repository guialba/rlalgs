{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# import pandas as pd\n",
    "\n",
    "from rlenvs import Hunting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição do Esimador:\n",
    "\n",
    "$\\hat{T}_m(s,a,k) = \\hat{T}_m(s,a,k) + \\Delta \\hat{T}_m(k) , \\; \\forall k \\in S$ \n",
    "\n",
    "$\n",
    "    \\Delta \\hat{T}_m(k) = \n",
    "        \\begin{cases}\n",
    "            \\frac{1- \\hat{T}_m(s,a,k) }{N_m(s,a)+1} , & \\text{if } k = s'\\\\\n",
    "            \\\\\n",
    "            \\frac{0- \\hat{T}_m(s,a,k) }{N_m(s,a)+1}, & \\text{if } k \\neq s'\n",
    "        \\end{cases} \n",
    "$  \n",
    "\n",
    "\n",
    "\n",
    "$\\hat{R}_m(s,a,s') = \\hat{R}_m(s,a,s') + \\Delta \\hat{R}_m $ \n",
    "\n",
    "$\\Delta \\hat{R}_m = \\frac{r- \\hat{R}_m(s,a) }{N_m(s,a)+1}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualidade Instantanea por estimador\n",
    "\n",
    "$e^T_{m} = 1-2(Z_T \\sum_{k \\in S}\\Delta\\hat{T}_m(k)^2)$ \n",
    "\n",
    "$e^R_{m} = 1-2(Z_R \\Delta\\hat{R}_m^2)$ \n",
    "\n",
    "\n",
    "$Z_T= \\frac{1}{2}(N(s,a)+1)^2$ \\\n",
    "$Z_R= (R_{\\max}-R_{\\min})^{-1} = (0 - (-1))^{-1} = 1$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator:\n",
    "    def __init__(self, M=1e6, *inputs):\n",
    "        self.M = M\n",
    "        self.n = np.zeros(inputs)     \n",
    "        self.v = np.zeros(inputs) \n",
    "        # self.e = 0\n",
    "\n",
    "    def e(self, value=1, *inputs):\n",
    "        return 1-2 * (self.z(*inputs) * self.delta(value, *inputs)**2)\n",
    "\n",
    "    def z(self, *inputs):\n",
    "        return 1\n",
    "\n",
    "    def delta(self, value, *inputs):\n",
    "        return (value-self.v[inputs]) / (np.sum(self.n[inputs[:-1]])+1)\n",
    "\n",
    "    def train(self, value=1, *inputs):\n",
    "        self.n[inputs] = min(self.n[inputs]+1, self.M)\n",
    "        delta = self.delta(value, *inputs)\n",
    "        self.v[inputs] += delta\n",
    "        \n",
    "    def forward(self, *inputs):\n",
    "        return self.v[inputs]\n",
    "    \n",
    "    \n",
    "    \n",
    "est = Estimator((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator_T(Estimator):    \n",
    "    def e(self, value=1, *inputs):\n",
    "        return 1-2*  self.z(*inputs)  * np.sum(self.delta(value, *inputs)**2)\n",
    "    \n",
    "    def z(self, *inputs):\n",
    "        s,a,_ = inputs\n",
    "        return (1/2)*(np.sum(self.n[s,a])+1)**2    \n",
    "\n",
    "    def delta(self, value=1, *inputs):\n",
    "        S = self.n.shape[0]\n",
    "        s,a,s_ = inputs\n",
    "        d = lambda k: (\n",
    "                (value-self.v[s,a,k])/(np.sum(self.n[s,a])+1) \n",
    "                    if k==s_ else \n",
    "                (0-self.v[s,a,k])/(np.sum(self.n[s,a])+1)\n",
    "            )\n",
    "        return np.array([d(k) for k in range(S)])\n",
    "\n",
    "    \n",
    "    def train(self, value=1, *inputs):\n",
    "        S = self.n.shape[0]\n",
    "        s,a,s_ = inputs\n",
    "\n",
    "        self.n[inputs] = min(self.n[inputs]+1, self.M)\n",
    "        delta = self.delta(value, *inputs)\n",
    "        for k in range(S):\n",
    "            self.v[s,a,k] += delta[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n",
    "#### Qualidade Instantanea modelo:  \n",
    "\n",
    "\n",
    "$E_m = E_m + \\rho(e_m - E_m)$\n",
    "\n",
    "$\\rho = 1$\n",
    "\n",
    "$e_m = c_m(s,a) (\\Omega e^R_m + (1-\\Omega)e^T_m)$\n",
    "\n",
    "$c_m(s,a) = \\frac{N_m(s,a)}{M}$  \n",
    "$\\Omega = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, S,A, Omega=.5, M=1e2, rho=.9):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Omega = Omega\n",
    "        self.M = M\n",
    "        self.rho = rho\n",
    "        self.N = 0\n",
    "\n",
    "        self._E = 0 \n",
    "\n",
    "        self.t = Estimator_T(self.M, len(self.S), len(self.A) ,len(self.S))\n",
    "        self.r = Estimator(self.M, len(self.S), len(self.A) ,len(self.S))\n",
    "\n",
    "    def e(self, s,a,s_,r):\n",
    "        nm = min(np.sum(self.t.n[s,a])+1, self.t.M)\n",
    "        cm = nm/self.t.M\n",
    "        return cm*(self.Omega * self.r.e(r, s,a,s_) + (1-self.Omega) * self.t.e(1, s,a,s_))\n",
    "    \n",
    "    def E(self, s,a,s_,r):\n",
    "        self._E += self.rho*(self.e(s,a,s_,r) - self._E)\n",
    "        return self._E\n",
    "\n",
    "    def learn(self, s,a,s_,r):\n",
    "        self.N += 1 \n",
    "        self.t.train(1, s,a,s_)\n",
    "        self.r.train(r, s,a,s_)\n",
    "    \n",
    "    def simulate(self, s, a): \n",
    "        s_ = np.random.choice(len(self.S), p=softmax(self.t.forward(s,a)))\n",
    "        r = self.r.forward(s,a,s_)\n",
    "        return s_,r\n",
    "        \n",
    "    def T(self, s,a,s_): # predict_T\n",
    "        return self.t.forward(s,a,s_)\n",
    "    def R(self, s,a,s_):\n",
    "        return self.r.forward(s,a,s_)\n",
    "\n",
    "model = Model(range(10), range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLCD:\n",
    "    def __init__(self, S,A, Emin=-0.01, Omega=0, M=1e2):\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.Models = []\n",
    "        self.current_model = None\n",
    "        self.Emin = Emin\n",
    "        self.Omega = Omega\n",
    "        self.M = M\n",
    "        self.new_model()\n",
    "\n",
    "    def new_model(self, M=1e2):\n",
    "        self.current_model = Model(self.S, self.A, self.Omega, self.M)\n",
    "        self.Models.append(self.current_model)\n",
    "\n",
    "    def learn(self, s,a,s_,r, log=False):\n",
    "        E = [m.E(s,a,s_,r) for m in self.Models]\n",
    "        self.current_model = self.Models[ np.argmax(E) ]\n",
    "        new_model = self.current_model._E < self.Emin\n",
    "        \n",
    "        if log:\n",
    "            print('E: ', E, new_model)\n",
    "        if new_model:\n",
    "            self.new_model()\n",
    "\n",
    "        self.current_model.learn(s,a,s_,r)\n",
    "    \n",
    "    def simulate(self, s, a):\n",
    "        sims = [m.simulate(s,a) for m in self.Models]\n",
    "        E = [m.E(s,a,s_,r) for m, (s_,r) in zip(self.Models, sims)]\n",
    "        return sims[ np.argmax(E) ]\n",
    "\n",
    "\n",
    "model = RLCD(range(10), range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorítimo de Controle (RL):\n",
    " - Dyna Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dyna:\n",
    "    def __init__(self, model, n=100, alpha=.9, gamma=.9, epsilon=.1):\n",
    "        self.model = model\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = np.zeros((self.model.S.size, self.model.A.size))\n",
    "        self.hist = np.zeros((self.model.S.size, self.model.A.size))\n",
    "\n",
    "    def run(self,s,a):\n",
    "        self.hist[s,a] += 1\n",
    "\n",
    "        Ss = np.random.choice([i for i,v in enumerate(np.sum(self.hist, axis=1)) if v>0], self.n) # n random seen states \n",
    "        for s in Ss:\n",
    "            a = np.random.choice([i for i,v in enumerate(self.hist[s]) if v>0]) # a random taken action in state s\n",
    "            s_,r = self.model.simulate(s,a)\n",
    "            self.Q[s,a] += self.alpha*(r + self.gamma*np.max(self.Q[s_]) - self.Q[s,a]) \n",
    "\n",
    "    def v(self):\n",
    "        return np.max(self.Q, axis=1)\n",
    "    def control(self):\n",
    "        return np.argmax(self.Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, S, A, n=100, alpha=.9 ,gamma=.9, epsilon=.1):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.model = RLCD(S, A)\n",
    "        self.rl = Dyna(self.model, self.n, self.alpha, self.gamma, self.epsilon)\n",
    "    \n",
    "    def learn(self, s,a,s_,r):\n",
    "        self.model.learn(s,a,s_,r)\n",
    "        self.rl.run(s,a)\n",
    "\n",
    "    def act(self, s):\n",
    "        pi = self.get_policy()\n",
    "        return pi[s]\n",
    "    def evaluate(self, s):\n",
    "        v = self.get_v()\n",
    "        return v[s]\n",
    "    \n",
    "    def get_v(self):\n",
    "        return self.rl.v()\n",
    "    def get_policy(self):\n",
    "        return self.rl.control()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env,agent, size_limit=100):\n",
    "    data = []\n",
    "    env.reset()\n",
    "    for _ in range(size_limit):\n",
    "        s, _, _, _, _ = env.last()\n",
    "        a = agent.act(s)\n",
    "        s_, r, end, _, _ = env.step(s,a)\n",
    "        \n",
    "        step = (s,a,s_,r) \n",
    "        agent.learn(*step) \n",
    "        data.append(step)\n",
    "        if end: break\n",
    "    return data\n",
    "\n",
    "def experiment(env, agent, max_iterations=1000, episode_sizes=100):\n",
    "    data = []\n",
    "    for _ in range(max_iterations):\n",
    "        epi = generate_episode(env, agent, episode_sizes)\n",
    "        data.append(len(epi))    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Hunting(5,5)\n",
    "\n",
    "n = 100\n",
    "alpha = 0.9\n",
    "epsilon = 0.1\n",
    "gamma = 0.9\n",
    "num_episodes = 100\n",
    "episode_size = 12\n",
    "\n",
    "agent = Agent(env.S, env.A, n, alpha, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = experiment(env, agent, num_episodes, episode_size)\n",
    "# exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _____________________________ \n",
      "| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
      "|_____|_____|_____|_____|_____|\n",
      "| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
      "|_____|_____|_____|_____|_____|\n",
      "| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
      "|_____|_____|_____|_____|_____|\n",
      "| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
      "|_____|_____|_____|_____|_____|\n",
      "| 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n",
      " _____________________________ \n",
      "|  ↑  |  ↓  |  ←  |  →  |  ↑  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↑  |  →  |  ←  |  →  |  ↓  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↑  |  ↓  |  ↓  |  ↓  |  ↑  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↑  |  →  |  ↑  |  →  |  ↓  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "|  ↑  |  ↓  |  ↑  |  ↑  |  ↑  |\n",
      "|_____|_____|_____|_____|_____|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.plot(list(np.round(agent.get_v(), 2)))\n",
    "env.plot(list(agent.get_policy()), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.model.T(0,0,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
